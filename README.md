# ğŸ“˜ **DÃ©tection dâ€™attaques DDoS par Apprentissage par Renforcement (PPO vs Q-Learning)**

---

## ğŸ”· 1. Introduction

Ce projet vise Ã  comparer lâ€™efficacitÃ© de deux algorithmes dâ€™apprentissage par renforcement (RL) â€“ **PPO (Proximal Policy Optimization)** et **Q-Learning** â€“ pour la dÃ©tection dâ€™attaques DDoS dans un environnement simulÃ© de rÃ©seau.

---

## ğŸ”· 2. Objectifs

- Concevoir un environnement simulant des attaques DDoS.
- ImplÃ©menter et entraÃ®ner des agents RL avec PPO et Q-Learning.
- Comparer leurs performances en termes de dÃ©tection, prÃ©cision et temps dâ€™apprentissage.

---

## ğŸ”· 3. PrÃ©requis

- Python 3.8+
- pip

---

## ğŸ”· Dataset CIC-DDoS2019

Le projet utilise le dataset CIC-DDoS2019 (Canadian Institute for Cybersecurity).
Le tÃ©lÃ©chargement se fait automatiquement via kagglehub :

python -m src.data.download_cicddos2019

---

## ğŸ”· 4. Installation

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/votre-utilisateur/ddos_rl_benchmark.git
   cd ddos_rl_benchmark
   ```
2. **CrÃ©er un environnement virtuel (optionnel mais recommandÃ©) :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```
3. **Installer les dÃ©pendances :**
   ```bash
   pip install -r requirements.txt
   ```
4. **TÃ©lÃ©charger le dataset :**
   ```bash
   python -m src.data.download_cicddos2019
   ```

---

## ğŸ”· 5. Structure du projet

```
ddos_rl_benchmark/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ envs/
â”‚   â””â”€â”€ data/
â”‚â”€â”€ main.py
â”‚â”€â”€ notes.md
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
```

*Note : Avant dâ€™entraÃ®ner les agents, il est nÃ©cessaire de prÃ©traiter les donnÃ©es.*

### ğŸ”· Phase 2 â€” PrÃ©traitement

Avant lâ€™entraÃ®nement des agents, lancer le pipeline de prÃ©traitement :
```
python -m src.data.preprocessing
```
Cela gÃ©nÃ¨re automatiquement les fichiers normalisÃ©s dans `data/processed/`.

---

## ğŸ”· 6. Utilisation

âš ï¸ *Cette section sera mise Ã  jour lorsque les scripts dâ€™entraÃ®nement
(PPO et Q-Learning) seront finalisÃ©s.*

Les commandes ci-dessous sont indicatives et seront ajustÃ©es :

### Lancer une expÃ©rience PPO :
```bash
python main.py --algo ppo --episodes 1000
```

### Lancer une expÃ©rience Q-Learning :
```bash
python main.py --algo qlearning --episodes 1000
```

### Options principales :
- `--algo` : Choix de lâ€™algorithme (`ppo` ou `qlearning`)
- `--episodes` : Nombre dâ€™Ã©pisodes dâ€™entraÃ®nement
- `--render` : Affiche lâ€™environnement (si applicable)

---

## ğŸ”· 7. RÃ©sultats attendus

- **Courbes dâ€™apprentissage** : PrÃ©cision, taux de dÃ©tection, taux de faux positifs.
- **Comparaison** : Tableaux comparatifs entre PPO et Q-Learning.
- **ReproductibilitÃ©** : Scripts et seeds pour rÃ©pÃ©ter les expÃ©riences.

---

## ğŸ”· 8. RÃ©fÃ©rences

- [OpenAI Gym](https://gym.openai.com/)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
- [Introduction to Reinforcement Learning (Sutton & Barto)](http://incompleteideas.net/book/the-book.html)

---

## ğŸ”· 9. Auteurs

- **Nathan HÃ©rault** â€“ UQO
- **BafodÃ© Koulibaly** â€“ UQO

---

## ğŸ”· 10. Licence

Ce projet est sous licence MIT.